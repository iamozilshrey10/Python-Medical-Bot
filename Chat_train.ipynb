{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chat_train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7uWv_KxDZYY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "e4c5d144-6775-4bd5-cb29-1a50860d1490"
      },
      "source": [
        "# Importing Modules \n",
        "\n",
        "import nltk\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import pickle\n",
        "import json\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['!','?']\n",
        "data = open('intents.json').read()\n",
        "intents = json.loads(data)\n",
        "\n",
        "print(intents)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'intents': [{'tag': 'greeting', 'patterns': ['Hi there', 'How are you', 'Is anyone there?', 'Hey', 'Hola', 'Hello', 'Good day'], 'responses': ['Hello, thanks for asking', 'Good to see you again', 'Hi there, how can I help?'], 'context': ['']}, {'tag': 'goodbye', 'patterns': ['Bye', 'See you later', 'Goodbye', 'Nice chatting to you, bye', 'Till next time'], 'responses': ['See you!', 'Have a nice day', 'Bye! Come back again soon.'], 'context': ['']}, {'tag': 'thanks', 'patterns': ['Thanks', 'Thank you', \"That's helpful\", 'Awesome, thanks', 'Thanks for helping me'], 'responses': ['Happy to help!', 'Any time!', 'My pleasure'], 'context': ['']}, {'tag': 'noanswer', 'patterns': [], 'responses': [\"Sorry, can't understand you\", 'Please give me more info', 'Not sure I understand'], 'context': ['']}, {'tag': 'options', 'patterns': ['How you could help me?', 'What you can do?', 'What help you provide?', 'How you can be helpful?', 'What support is offered'], 'responses': ['I can guide you through Adverse drug reaction list, Blood pressure tracking, Hospitals and Pharmacies', 'Offering support for Adverse drug reaction, Blood pressure, Hospitals and Pharmacies'], 'context': ['']}, {'tag': 'adverse_drug', 'patterns': ['How to check Adverse drug reaction?', 'Open adverse drugs module', 'Give me a list of drugs causing adverse behavior', 'List all drugs suitable for patient with adverse reaction', 'Which drugs dont have adverse reaction?'], 'responses': ['Navigating to Adverse drug reaction module'], 'context': ['']}, {'tag': 'blood_pressure', 'patterns': ['Open blood pressure module', 'Task related to blood pressure', 'Blood pressure data entry', 'I want to log blood pressure results', 'Blood pressure data management'], 'responses': ['Navigating to Blood Pressure module'], 'context': ['']}, {'tag': 'blood_pressure_search', 'patterns': ['I want to search for blood pressure result history', 'Blood pressure for patient', 'Load patient blood pressure result', 'Show blood pressure results for patient', 'Find blood pressure results by ID'], 'responses': ['Please provide Patient ID', 'Patient ID?'], 'context': ['search_blood_pressure_by_patient_id']}, {'tag': 'search_blood_pressure_by_patient_id', 'patterns': [], 'responses': ['Loading Blood pressure result for Patient'], 'context': ['']}, {'tag': 'pharmacy_search', 'patterns': ['Find me a pharmacy', 'Find pharmacy', 'List of pharmacies nearby', 'Locate pharmacy', 'Search pharmacy'], 'responses': ['Please provide pharmacy name'], 'context': ['search_pharmacy_by_name']}, {'tag': 'search_pharmacy_by_name', 'patterns': [], 'responses': ['Loading pharmacy details'], 'context': ['']}, {'tag': 'hospital_search', 'patterns': ['Lookup for hospital', 'Searching for hospital to transfer patient', 'I want to search hospital data', 'Hospital lookup for patient', 'Looking up hospital details'], 'responses': ['Please provide hospital name or location'], 'context': ['search_hospital_by_params']}, {'tag': 'search_hospital_by_params', 'patterns': [], 'responses': ['Please provide hospital type'], 'context': ['search_hospital_by_type']}, {'tag': 'search_hospital_by_type', 'patterns': [], 'responses': ['Loading hospital details'], 'context': ['']}]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwTu32_hEhK9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprossing the data\n",
        "\n",
        "for i in intents['intents']:\n",
        "  for j in i['patterns']:\n",
        "\n",
        "    w = nltk.word_tokenize(j)   #Tokeinzing each word \n",
        "    words.extend(w)\n",
        "\n",
        "    documents.append((w, i['tag']))  #Adding documents \n",
        "\n",
        "    if i['tag'] not in classes:\n",
        "      classes.append(i['tag'])    #Adding to the classes list"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ0lvW1FGmlP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lemmatizing and discarding duplicate words \n",
        "\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "classes = sorted(list(set(classes))) #Sorting classes \n",
        "\n",
        "#print(len(documents),\"Documents\") #Combination between patterns and intents \n",
        "#print(len(classes), \"Classes\", classes) classes = intents \n",
        "#print(len(words), \"Lematized Words\", words) #all words, vocab\n",
        "\n",
        "pickle.dump(words, open('Words.pkl','wb'))\n",
        "pickle.dump(classes, open('Classes.pkl','wb'))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_XemK0OJHoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating Training and Testing Data\n",
        "\n",
        "training = []\n",
        "out = [0] * len(classes) #empty array for output\n",
        "\n",
        "for doc in documents:\n",
        "  bag = []  #initilize bag of words \n",
        "  pattern_words = doc[0] #tokenized words for the pattern\n",
        "  pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "\n",
        "  #If match found in current pattern create our bag of words array with 1\n",
        "  for w in words:\n",
        "    bag.append(1) if w in pattern_words else bag.append(0)\n",
        "    out_row = list(out)\n",
        "    out_row[classes.index(doc[1])] = 1\n",
        "    training.append([bag, out_row])\n",
        "\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "X_train = list(training[:,0])\n",
        "y_train = list(training[:,1])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78dJd0fyM6La",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Building the Model \n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(X_train[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(y_train[0]), activation = 'softmax'))\n",
        "\n",
        "#Compling the Model\n",
        "\n",
        "sgd = SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
        "\n",
        "#Fitting and Saving the Model \n",
        "hist = model.fit(np.array(X_train), np.array(y_train), epochs = 200, batch_size = 5, verbose = 1)\n",
        "model.save('chat_model.h5',hist)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}